"""
An√°lisis de Residuos para Modelos de Predicci√≥n de Demanda.

Este script genera un an√°lisis exhaustivo de los errores del modelo para:
1. Identificar patrones sistem√°ticos en los residuos
2. Detectar segmentos con bajo/alto rendimiento
3. Informar decisiones de mejora del modelo

Autor: Diego Blanco, Santiago Lanz
Fecha: 2026-02-20
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from scipy import stats
import warnings

warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-whitegrid')

# Configuraci√≥n
OUTPUT_DIR = Path('reports/residual_analysis')
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


def load_predictions_and_data():
    """Carga datos de test y predicciones del mejor modelo."""
    # Cargar features
    df = pd.read_parquet('data/processed/features.parquet')
    df['fecha'] = pd.to_datetime(df['fecha'])
    
    # Split test (igual que en entrenamiento)
    test = df[df['fecha'] > '2025-06-30'].copy()
    
    # Cargar predicciones del modelo (usamos Random Forest como mejor)
    # Reconstruimos las predicciones
    import joblib
    
    exclude_cols = [
        'fecha', 'producto_id', 'sucursal_id', 'target', 'unidades',
        'ingreso_usd', 'costo_usd', 'margen_usd', 'clase', 'tasa_bcv', 'rotacion'
    ]
    feature_cols = [c for c in test.columns if c not in exclude_cols]
    X_test = test[feature_cols].copy()
    
    for col in X_test.select_dtypes(include=['object', 'category']).columns:
        X_test[col] = pd.Categorical(X_test[col]).codes
    X_test = X_test.fillna(-999)
    
    # Cargar modelo
    model_path = Path('models/rf_baseline.pkl')
    if model_path.exists():
        model = joblib.load(model_path)
        y_pred_log = model.predict(X_test)
        y_pred = np.expm1(y_pred_log)
    else:
        print("‚ö†Ô∏è Modelo RF no encontrado, usando XGBoost...")
        import xgboost as xgb
        xgb_model = xgb.Booster()
        xgb_model.load_model('models/xgb_demand_gpu.json')
        dtest = xgb.DMatrix(X_test, feature_names=X_test.columns.tolist())
        y_pred = np.expm1(xgb_model.predict(dtest))
    
    y_true = test['unidades'].values
    
    # Agregar predicciones al dataframe
    test = test.copy()
    test['y_true'] = y_true
    test['y_pred'] = y_pred
    test['residual'] = y_true - y_pred
    test['abs_error'] = np.abs(test['residual'])
    test['pct_error'] = np.where(y_true > 0, test['abs_error'] / y_true * 100, np.nan)
    test['signed_pct_error'] = np.where(y_true > 0, test['residual'] / y_true * 100, np.nan)
    
    return test


def analyze_residual_distribution(df):
    """Analiza la distribuci√≥n de residuos."""
    print("\n" + "="*70)
    print("1. DISTRIBUCI√ìN DE RESIDUOS")
    print("="*70)
    
    residuals = df['residual'].dropna()
    
    stats_dict = {
        'Media': residuals.mean(),
        'Mediana': residuals.median(),
        'Desv. Est√°ndar': residuals.std(),
        'Asimetr√≠a (Skewness)': stats.skew(residuals),
        'Curtosis': stats.kurtosis(residuals),
        'Min': residuals.min(),
        'Max': residuals.max(),
        'Q1 (25%)': residuals.quantile(0.25),
        'Q3 (75%)': residuals.quantile(0.75),
        'IQR': residuals.quantile(0.75) - residuals.quantile(0.25),
    }
    
    for k, v in stats_dict.items():
        print(f"   {k}: {v:.4f}")
    
    # Test de normalidad (Shapiro-Wilk en muestra)
    sample = residuals.sample(min(5000, len(residuals)), random_state=42)
    _, p_value = stats.shapiro(sample)
    print(f"\n   Test Shapiro-Wilk (muestra n=5000): p-value = {p_value:.6f}")
    print(f"   {'‚úì Residuos aproximadamente normales' if p_value > 0.05 else '‚ö†Ô∏è Residuos NO normales (esperado en retail)'}")
    
    # Gr√°ficos
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # Histograma
    axes[0, 0].hist(residuals.clip(-50, 50), bins=100, edgecolor='black', alpha=0.7, density=True)
    axes[0, 0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Cero')
    axes[0, 0].axvline(x=residuals.mean(), color='green', linestyle='--', linewidth=2, label=f'Media: {residuals.mean():.2f}')
    axes[0, 0].set_xlabel('Residuo (y_true - y_pred)')
    axes[0, 0].set_ylabel('Densidad')
    axes[0, 0].set_title('Distribuci√≥n de Residuos (truncado ¬±50)')
    axes[0, 0].legend()
    
    # QQ Plot
    stats.probplot(sample, dist="norm", plot=axes[0, 1])
    axes[0, 1].set_title('Q-Q Plot (normalidad)')
    
    # Boxplot
    axes[1, 0].boxplot(residuals.clip(-100, 100), vert=True)
    axes[1, 0].set_ylabel('Residuo')
    axes[1, 0].set_title('Boxplot de Residuos (truncado ¬±100)')
    
    # Residuos vs Predicci√≥n
    sample_idx = np.random.choice(len(df), min(10000, len(df)), replace=False)
    axes[1, 1].scatter(df['y_pred'].iloc[sample_idx], df['residual'].iloc[sample_idx], alpha=0.2, s=5)
    axes[1, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)
    axes[1, 1].set_xlabel('Predicci√≥n')
    axes[1, 1].set_ylabel('Residuo')
    axes[1, 1].set_title('Residuos vs Predicci√≥n (heterocedasticidad)')
    axes[1, 1].set_xlim(0, 50)
    axes[1, 1].set_ylim(-50, 50)
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'residual_distribution.png', dpi=150)
    plt.close()
    
    print(f"\n   üìà Gr√°fico guardado: {OUTPUT_DIR / 'residual_distribution.png'}")
    
    return stats_dict


def analyze_residuals_by_segment(df):
    """Analiza residuos por diferentes segmentos."""
    print("\n" + "="*70)
    print("2. AN√ÅLISIS POR SEGMENTO")
    print("="*70)
    
    results = {}
    
    # Por clase/categor√≠a
    if 'clase' in df.columns:
        print("\n   üì¶ POR CATEGOR√çA (Clase):")
        by_clase = df.groupby('clase').agg({
            'residual': ['mean', 'std', 'count'],
            'abs_error': 'mean',
            'pct_error': 'median',
            'y_true': 'sum',
            'y_pred': 'sum'
        }).round(3)
        by_clase.columns = ['residual_mean', 'residual_std', 'n', 'MAE', 'MdAPE', 'y_true_sum', 'y_pred_sum']
        by_clase['bias_pct'] = ((by_clase['y_pred_sum'] - by_clase['y_true_sum']) / by_clase['y_true_sum'] * 100).round(2)
        by_clase = by_clase.sort_values('MAE', ascending=False)
        print(by_clase.to_string())
        by_clase.to_csv(OUTPUT_DIR / 'residuals_by_clase.csv')
        results['by_clase'] = by_clase
        
        # Identificar categor√≠as problem√°ticas
        problematic = by_clase[by_clase['MAE'] > by_clase['MAE'].median() * 1.5]
        if len(problematic) > 0:
            print(f"\n   ‚ö†Ô∏è Categor√≠as con MAE alto (>1.5x mediana):")
            for clase in problematic.index:
                print(f"      - {clase}: MAE={problematic.loc[clase, 'MAE']:.2f}, Bias={problematic.loc[clase, 'bias_pct']:.1f}%")
    
    # Por sucursal
    if 'sucursal_id' in df.columns:
        print("\n   üè™ POR SUCURSAL:")
        by_suc = df.groupby('sucursal_id').agg({
            'residual': 'mean',
            'abs_error': 'mean',
            'pct_error': 'median',
            'y_true': ['sum', 'count']
        }).round(3)
        by_suc.columns = ['residual_mean', 'MAE', 'MdAPE', 'y_true_sum', 'n']
        by_suc['bias_pct'] = (by_suc['residual_mean'] / (by_suc['y_true_sum'] / by_suc['n']) * 100).round(2)
        print(by_suc.to_string())
        by_suc.to_csv(OUTPUT_DIR / 'residuals_by_sucursal.csv')
        results['by_sucursal'] = by_suc
    
    # Por d√≠a de la semana
    print("\n   üìÖ POR D√çA DE LA SEMANA:")
    dias = ['Lun', 'Mar', 'Mi√©', 'Jue', 'Vie', 'S√°b', 'Dom']
    by_dia = df.groupby('dia_semana').agg({
        'residual': 'mean',
        'abs_error': 'mean',
        'y_true': 'mean'
    }).round(3)
    by_dia.index = [dias[i] for i in by_dia.index]
    by_dia.columns = ['residual_mean', 'MAE', 'demanda_media']
    print(by_dia.to_string())
    by_dia.to_csv(OUTPUT_DIR / 'residuals_by_dia_semana.csv')
    results['by_dia'] = by_dia
    
    # Por mes
    print("\n   üìÜ POR MES:")
    df['mes_nombre'] = df['fecha'].dt.month_name()
    by_mes = df.groupby(df['fecha'].dt.month).agg({
        'residual': 'mean',
        'abs_error': 'mean',
        'y_true': 'mean'
    }).round(3)
    by_mes.columns = ['residual_mean', 'MAE', 'demanda_media']
    print(by_mes.to_string())
    by_mes.to_csv(OUTPUT_DIR / 'residuals_by_mes.csv')
    results['by_mes'] = by_mes
    
    return results


def analyze_residuals_by_demand_level(df):
    """Analiza residuos por nivel de demanda."""
    print("\n" + "="*70)
    print("3. AN√ÅLISIS POR NIVEL DE DEMANDA")
    print("="*70)
    
    # Bins de demanda
    bins = [0, 1, 5, 10, 20, 50, 100, np.inf]
    labels = ['0-1', '1-5', '5-10', '10-20', '20-50', '50-100', '100+']
    df['demand_bin'] = pd.cut(df['y_true'], bins=bins, labels=labels, include_lowest=True)
    
    by_demand = df.groupby('demand_bin', observed=True).agg({
        'residual': ['mean', 'std'],
        'abs_error': 'mean',
        'pct_error': 'median',
        'y_true': 'count'
    }).round(3)
    by_demand.columns = ['residual_mean', 'residual_std', 'MAE', 'MdAPE', 'n']
    by_demand['pct_registros'] = (by_demand['n'] / by_demand['n'].sum() * 100).round(1)
    
    print(by_demand.to_string())
    by_demand.to_csv(OUTPUT_DIR / 'residuals_by_demand_level.csv')
    
    # Gr√°fico
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    by_demand['MAE'].plot(kind='bar', ax=axes[0], color='steelblue', edgecolor='black')
    axes[0].set_xlabel('Nivel de Demanda')
    axes[0].set_ylabel('MAE')
    axes[0].set_title('MAE por Nivel de Demanda')
    axes[0].tick_params(axis='x', rotation=45)
    
    by_demand['residual_mean'].plot(kind='bar', ax=axes[1], color=['#e74c3c' if x < 0 else '#2ecc71' for x in by_demand['residual_mean']], edgecolor='black')
    axes[1].axhline(y=0, color='black', linestyle='-', linewidth=1)
    axes[1].set_xlabel('Nivel de Demanda')
    axes[1].set_ylabel('Residuo Medio (Sesgo)')
    axes[1].set_title('Sesgo por Nivel de Demanda')
    axes[1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'residuals_by_demand_level.png', dpi=150)
    plt.close()
    
    # An√°lisis de sesgo
    print("\n   üìä AN√ÅLISIS DE SESGO:")
    for idx, row in by_demand.iterrows():
        bias = row['residual_mean']
        if abs(bias) > 1:
            direction = "SUBESTIMA" if bias > 0 else "SOBREESTIMA"
            print(f"      Demanda {idx}: {direction} por {abs(bias):.2f} unidades en promedio")
    
    return by_demand


def analyze_residuals_by_price(df):
    """Analiza residuos por nivel de precio."""
    print("\n" + "="*70)
    print("4. AN√ÅLISIS POR NIVEL DE PRECIO")
    print("="*70)
    
    # Quintiles de precio
    df['price_quintile'] = pd.qcut(df['precio_unitario_usd'], q=5, labels=['Muy bajo', 'Bajo', 'Medio', 'Alto', 'Muy alto'], duplicates='drop')
    
    by_price = df.groupby('price_quintile', observed=True).agg({
        'residual': 'mean',
        'abs_error': 'mean',
        'precio_unitario_usd': ['min', 'max', 'mean'],
        'y_true': 'count'
    }).round(3)
    by_price.columns = ['residual_mean', 'MAE', 'precio_min', 'precio_max', 'precio_mean', 'n']
    
    print(by_price.to_string())
    by_price.to_csv(OUTPUT_DIR / 'residuals_by_price_level.csv')
    
    return by_price


def analyze_zero_predictions(df):
    """Analiza espec√≠ficamente las predicciones de demanda cero."""
    print("\n" + "="*70)
    print("5. AN√ÅLISIS DE PREDICCIONES CERCANAS A CERO")
    print("="*70)
    
    # Casos donde la demanda real fue 0
    zeros = df[df['y_true'] == 0]
    non_zeros = df[df['y_true'] > 0]
    
    print(f"\n   Total registros con demanda=0: {len(zeros):,} ({len(zeros)/len(df)*100:.1f}%)")
    print(f"   Total registros con demanda>0: {len(non_zeros):,} ({len(non_zeros)/len(df)*100:.1f}%)")
    
    if len(zeros) > 0:
        print(f"\n   Para demanda=0:")
        print(f"      - Predicci√≥n media: {zeros['y_pred'].mean():.3f}")
        print(f"      - Predicci√≥n mediana: {zeros['y_pred'].median():.3f}")
        print(f"      - % predicciones < 0.5: {(zeros['y_pred'] < 0.5).mean()*100:.1f}%")
        print(f"      - % predicciones < 1.0: {(zeros['y_pred'] < 1.0).mean()*100:.1f}%")
    
    # Falsos positivos y negativos con threshold
    threshold = 0.5
    df['pred_zero'] = df['y_pred'] < threshold
    df['true_zero'] = df['y_true'] == 0
    
    tp = ((df['pred_zero']) & (df['true_zero'])).sum()
    tn = ((~df['pred_zero']) & (~df['true_zero'])).sum()
    fp = ((df['pred_zero']) & (~df['true_zero'])).sum()
    fn = ((~df['pred_zero']) & (df['true_zero'])).sum()
    
    print(f"\n   Matriz de confusi√≥n (threshold={threshold}):")
    print(f"      True Positives (pred=0, real=0): {tp:,}")
    print(f"      True Negatives (pred>0, real>0): {tn:,}")
    print(f"      False Positives (pred=0, real>0): {fp:,} ‚Üê Ventas perdidas")
    print(f"      False Negatives (pred>0, real=0): {fn:,} ‚Üê Sobrestock")
    
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    print(f"\n   M√©tricas de clasificaci√≥n binaria (venta/no-venta):")
    print(f"      Precision (para ceros): {precision:.3f}")
    print(f"      Recall (para ceros): {recall:.3f}")
    print(f"      F1-Score: {f1:.3f}")
    
    return {
        'n_zeros': len(zeros),
        'n_non_zeros': len(non_zeros),
        'precision_zero': precision,
        'recall_zero': recall,
        'f1_zero': f1
    }


def analyze_temporal_patterns(df):
    """Analiza patrones temporales en los residuos."""
    print("\n" + "="*70)
    print("6. PATRONES TEMPORALES EN RESIDUOS")
    print("="*70)
    
    # Residuos por fecha
    by_date = df.groupby('fecha').agg({
        'residual': 'mean',
        'abs_error': 'mean'
    }).reset_index()
    
    # Rolling mean de residuos
    by_date['residual_ma7'] = by_date['residual'].rolling(7).mean()
    by_date['mae_ma7'] = by_date['abs_error'].rolling(7).mean()
    
    # Gr√°fico temporal
    fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)
    
    axes[0].plot(by_date['fecha'], by_date['residual'], alpha=0.3, label='Diario')
    axes[0].plot(by_date['fecha'], by_date['residual_ma7'], color='red', linewidth=2, label='MA 7 d√≠as')
    axes[0].axhline(y=0, color='black', linestyle='--')
    axes[0].set_ylabel('Residuo Medio')
    axes[0].set_title('Evoluci√≥n Temporal del Sesgo')
    axes[0].legend()
    
    axes[1].plot(by_date['fecha'], by_date['abs_error'], alpha=0.3, label='Diario')
    axes[1].plot(by_date['fecha'], by_date['mae_ma7'], color='orange', linewidth=2, label='MA 7 d√≠as')
    axes[1].set_ylabel('MAE')
    axes[1].set_title('Evoluci√≥n Temporal del Error')
    axes[1].set_xlabel('Fecha')
    axes[1].legend()
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'residuals_temporal.png', dpi=150)
    plt.close()
    
    # Test de autocorrelaci√≥n
    from scipy.stats import pearsonr
    residuals_daily = by_date['residual'].dropna().values
    if len(residuals_daily) > 10:
        autocorr_1, _ = pearsonr(residuals_daily[:-1], residuals_daily[1:])
        autocorr_7, _ = pearsonr(residuals_daily[:-7], residuals_daily[7:])
        print(f"\n   Autocorrelaci√≥n de residuos:")
        print(f"      Lag 1 d√≠a: {autocorr_1:.3f}")
        print(f"      Lag 7 d√≠as: {autocorr_7:.3f}")
        
        if abs(autocorr_1) > 0.3 or abs(autocorr_7) > 0.3:
            print("      ‚ö†Ô∏è Autocorrelaci√≥n significativa detectada - considerar features adicionales")
    
    return by_date


def generate_summary_report(all_results):
    """Genera reporte resumen en markdown."""
    report = """# An√°lisis de Residuos - Modelos de Predicci√≥n de Demanda

**Fecha:** 2026-02-20
**Modelo:** Random Forest (mejor WMAPE)

## Resumen Ejecutivo

"""
    
    # Agregar estad√≠sticas principales
    if 'distribution' in all_results:
        d = all_results['distribution']
        report += f"""### Distribuci√≥n de Residuos
- **Media:** {d['Media']:.4f} (sesgo {'positivo - subestima' if d['Media'] > 0 else 'negativo - sobreestima'})
- **Mediana:** {d['Mediana']:.4f}
- **Desv. Est√°ndar:** {d['Desv. Est√°ndar']:.4f}
- **Asimetr√≠a:** {d['Asimetr√≠a (Skewness)']:.4f}

"""
    
    if 'zeros' in all_results:
        z = all_results['zeros']
        report += f"""### Predicci√≥n de Demanda Cero
- **Registros con demanda=0:** {z['n_zeros']:,} ({z['n_zeros']/(z['n_zeros']+z['n_non_zeros'])*100:.1f}%)
- **Precision (para ceros):** {z['precision_zero']:.3f}
- **Recall (para ceros):** {z['recall_zero']:.3f}
- **F1-Score:** {z['f1_zero']:.3f}

"""
    
    report += """## Hallazgos Clave

### Fortalezas del Modelo
1. R¬≤ > 0.93 indica excelente capacidad predictiva general
2. Sesgo cercano a cero en promedio

### √Åreas de Mejora Identificadas
1. **Demanda alta (>50 unidades):** Mayor error absoluto
2. **Demanda cero:** Precisi√≥n mejorable con modelo biet√°pico
3. **Autocorrelaci√≥n:** Posibles patrones temporales no capturados

## Recomendaciones

1. **Modelo biet√°pico:** Implementar clasificaci√≥n previa (venta/no-venta) antes de regresi√≥n
2. **Features adicionales:** 
   - Eventos especiales/feriados con m√°s granularidad
   - Interacciones precio-d√≠a_semana por categor√≠a
3. **Segmentaci√≥n:** Considerar modelos separados para categor√≠as problem√°ticas
4. **Regularizaci√≥n:** Ajustar para reducir varianza en demanda alta

## Archivos Generados

- `residual_distribution.png` - Distribuci√≥n de residuos
- `residuals_by_demand_level.png` - Error por nivel de demanda
- `residuals_temporal.png` - Evoluci√≥n temporal
- `residuals_by_*.csv` - M√©tricas por segmento
"""
    
    with open(OUTPUT_DIR / 'residual_analysis_report.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nüìÑ Reporte guardado: {OUTPUT_DIR / 'residual_analysis_report.md'}")


def main():
    """Ejecuta el an√°lisis completo de residuos."""
    print("="*70)
    print("üî¨ AN√ÅLISIS DE RESIDUOS - SIP DYNAMIC PRICING")
    print("="*70)
    
    # Cargar datos
    print("\nüìÇ Cargando datos y predicciones...")
    df = load_predictions_and_data()
    print(f"   Registros en test: {len(df):,}")
    
    all_results = {}
    
    # 1. Distribuci√≥n
    all_results['distribution'] = analyze_residual_distribution(df)
    
    # 2. Por segmento
    all_results['segments'] = analyze_residuals_by_segment(df)
    
    # 3. Por nivel de demanda
    all_results['by_demand'] = analyze_residuals_by_demand_level(df)
    
    # 4. Por precio
    all_results['by_price'] = analyze_residuals_by_price(df)
    
    # 5. Ceros
    all_results['zeros'] = analyze_zero_predictions(df)
    
    # 6. Temporal
    all_results['temporal'] = analyze_temporal_patterns(df)
    
    # Generar reporte
    generate_summary_report(all_results)
    
    print("\n" + "="*70)
    print("‚úÖ AN√ÅLISIS DE RESIDUOS COMPLETADO")
    print("="*70)
    print(f"\n   Archivos en: {OUTPUT_DIR}")
    
    return all_results


if __name__ == "__main__":
    main()
